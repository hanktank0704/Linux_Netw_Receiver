---
creator: êµ¬ì„±í˜„
type: Paper Review
created: 2022-07-12
---
<<[[Paper XDP Summary|previous]]>>

<aside> ğŸ‘€ ì†Œí”„íŠ¸ì›¨ì–´ì—ì„œ íŒ¨í‚· í”„ë¡œì„¸ì‹±ì€ ì»¤ë„ì„ bypass í•˜ëŠ” ê²ƒì„ â†’ bypassí•˜ë©´ ì‹œê°„ì´ ëœë“ ë‹¤ëŠ” ì¥ì ê³¼ í•¨ê»˜ appì„ ê¸°ëŠ¥ì ìœ¼ë¡œ ë‹¤ì‹œ êµ¬í˜„í•˜ê±°ë‚˜ ë³´ì•ˆì˜ ë¬¸ì œê°€ ìˆìŒ â†’ ê³ ë¡œ ë…¼ë¬¸ì—ì„œ XDPë¥¼ ì†Œê°œí•¨

- ì–´ë–»ê²Œ ê³ ì¹¨? â†’ ì»¤ë„ì˜ ë„¤íŠ¸ì›Œí¬ ìŠ¤íƒì„ ì²˜ë¦¬í•˜ê¸° ì§ì „ì— eBPF ì½”ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” XDPë¥¼ ë‘ 
    
- ë­˜ë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ”ë°? â†’ XDP driver hook + eBPF virtual machine + BPF map + eBPF verifier
    
- ê°ê°ì´ ë­í•˜ëŠ”ë°? â†’ XDP driver hookì€ ì»¤ë„ ë“¤ì–´ê°€ê¸° context objectë¥¼ ë³´ê³  ì–´ë””ë¡œ ë³´ë‚¼ ê²ƒì¸ì§€ ê²°ì •í•¨ â†’ eBPF virtual machineì€ í”„ë¡œê·¸ë¨ì„ ìœ ë™ì ìœ¼ë¡œ ë¡œë”©í•˜ê±°ë‚˜ ì¬ë¡œë”©í•¨ â†’ BPF mapì€ eBPF í”„ë¡œê·¸ë¨ë“¤ì´ë‚˜ user spaceì™€ì˜ ì†Œí†µì„ ì§€ì› â†’ eBPF verifierëŠ” í”„ë¡œê·¸ë¨ì´ ì»¤ë„ì— harmí•˜ëŠ” í–‰ë™ì„ í•˜ì§€ ì•Šë„ë¡ ensure
    
- ê·¸ë˜ì„œ ë­ê°€ ì¢‹ì•„ì§? â†’ í˜„ì¬ Linux packet processingë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ â†’ í˜„ì¬ OS kernel networkê³¼ compatibleí•´ì„œ ì•ˆì „ì„±ì„ í™•ë³´ â†’ ë”°ë¡œ íŠ¹ë³„í•œ í•˜ë“œì›¨ì–´ë‚˜ ìì› ì—†ì´ dynamicí•˜ê²Œ re-program ê°€ëŠ¥
    

</aside>

INDEX

## PERFORMANCE EVALUATION

---

> ì‹¤í—˜ í™˜ê²½ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. í•´ë‹¹ ë…¼ë¬¸ì€ 3ê°œì˜ metricì— ì§‘ì¤‘í–ˆëŠ”ë°, í•˜ë‚˜ëŠ” ìµœëŒ€ packet processing performanceë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ packet drop performanceì´ê³ , ë‚˜ë¨¸ì§€ 2ê°œëŠ” CPU usageì™€ packet forwarding performanceì…ë‹ˆë‹¤.

- Setting
    - Intel Xeon E5-1650 v4 CPU running at 3.60GHz with Intelâ€™s Data Direct I/O (DDIO)
    - 2 Mellanox ConnectX-5 Ex VPI dual-port 100Gbps network adapters
    - TRex packet generator
    - Linux kernel 4.18
- Focus on three metrics
    - Packet drop performance
    - CPU usage
    - Packet forwarding performance

### Packet Drop Performance

> í•´ë‹¹ ê·¸ë˜í”„ëŠ” ì½”ì–´ì˜ ê°œìˆ˜ì— ë”°ë¼ packet drop performanceë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. í•œ ì½”ì–´ì—ì„œ XDPì˜ baseline performanceëŠ” 24Mppsì´ê³  DPDKëŠ” 43.5Mppsì…ë‹ˆë‹¤. ë‘˜ ë‹¤ PCI busì˜ global perfomance limitì— ë„ë‹¬í•  ë•Œê¹Œì§€ linearlyí•˜ê²Œ ì„±ëŠ¥ì„ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. Linuxì˜ conntrack modeì€ í•œ ì½”ì–´ì˜ ì„±ëŠ¥ì´ 1.8Mppsì´ê³ , raw modeì—ì„œëŠ” ìµœëŒ€ 4.8Mppsì…ë‹ˆë‹¤. hardware bottleneckì´ ì—†ì„ ê²½ìš° Linux performanceëŠ” ì½”ì–´ì˜ ìˆ˜ì— ë”°ë¼ linearí•˜ê²Œ ì¦ê°€í•©ë‹ˆë‹¤.
![[Untitled(68).png]]


Figure 3 : Packet drop performance

- The baseline performance of XDP for a single core is 24 Mpps for DPDK it is 43.5 Mpps
    - Both scale their performance linearly
    - until they approach the global performance limit of the PCI bus (reached at 115Mpps)
- with conntrack from 1.8Mpps of single-core performance, up to 4.8 Mpps in raw mode
    - Linux performance scales linearly with the number of cores

### CPU Usage

> DPDKê°€ packet processingê³¼ busy pollingìœ¼ë¡œ ëª¨ë“  ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— CPU usageê°€ í•­ìƒ 100%ì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ, XDPì™€ LinuxëŠ” smoothí•˜ê²Œ CPU usageë¥¼ scaleí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
![[Untitled(69).png]]


Figure 4 : CPU usage in the drop scenario

- DPDKâ€™s CPU usage is always pegged at 100%
    - since DPDK dedicates a full core to packet processing and uses busy polling
- XDP and Linux smoothly scale CPU usage
- The non-linearity in the bottom-left corner is due to the fixed overhead of interrupt processing

### Packet Forwarding Performance

> XDPëŠ” ë‹¤ë¥¸ NIC ë¿ë§Œ ì•„ë‹ˆë¼ ê°™ì€ NICìœ¼ë¡œë„ packetì„ forwardí•  ìˆ˜ ìˆì–´ ë‘ê°€ì§€ë¥¼ ë„£ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. DPDKëŠ” ë‹¤ë¥¸ interfaceë¡œë§Œ packet forwardë¥¼ ì§€ì›í•©ë‹ˆë‹¤. LinuxëŠ” minimal forwarding modeë¥¼ ì§€ì›í•˜ì§€ ì•Šì•„ ìƒëµí–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ê°™ì€ interfaceë¡œ packetì„ ë³´ë‚¼ ë•Œ XDP ì„±ëŠ¥ì´ í–¥ìƒëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
![[Untitled(70).png]]


Figure 5 : Packet forwarding throughput

- XDP can forward packets out the same NIC as well as out a different NIC
- DPDK only supports forwarding packets through a different interface
- Linux networking stack does not support minimal forwarding mode
    - but requires a full bridging or routing lookup
    - omit it from the results
- XDP performance improves, surpassing the DPDK forwarding at two cores and above
    - due to differences in memory handling

## CONCLUSION

---

> XDPëŠ” ë¹ ë¥¸ programmable packet processingê³¼ OS ì»¤ë„ì„ ì•ˆì „í•˜ê²Œ í•©ì¹œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. XDPëŠ” raw packet processing performanceì—ì„œ í•œ ì½”ì–´ì— ëŒ€í•´ ìµœëŒ€ 24Mppsë¥¼ ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì»¤ë„ì˜ ì•ˆì „ê³¼ management compatibilityë¥¼ í¬í•¨í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒí–ˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, XDPëŠ” dynamicí•˜ê²Œ service interruptionì—†ì´ re-programí•  ìˆ˜ ìˆê³  íŠ¹ë³„í•œ í•˜ë“œì›¨ì–´ë‚˜ resourceë¥¼ í•„ìš”ë¡œ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

- XDP, a system for safely integrating fast programmable packet processing into the OS kernel
- achieves raw packet processing performance of up to 24 Mpps on a single CPU core

## More â€¦

---

- ~~packet drop performanceê°€ ë†’ìœ¼ë©´ ì¢‹ì€ ê±´ê°€ìš”?~~
    - measure the performance of the simplest possible operation of dropping the incoming packet. this effectively measures the overhead of the system as a whole â€¦ [PERFORMANCE EVALUATION](https://www.notion.so/PERFORMANCE-EVALUATION-528cc92d7ef244b6abf8d8e261428acc?pvs=21)
- PCI busì˜ global performance limitê°€ ì˜ ì´í•´ê°€ ì•ˆë¼ìš”
    - global performance limit of the PCI bus, which is reached at 115Mpps after enabling PCI descriptor compression support in the hardware [Packet Drop Performance](https://www.notion.so/Packet-Drop-Performance-f501f70287704d80a0e5ca34c0d4bbdf?pvs=21)
- ì‘ì€ packet ratesì—ì„œ ì¸í„°ëŸ½íŠ¸ë„ ì‘ì€ë° ì™œ íŒ¨í‚·ë‹¹ CPU usageê°€ ë†’ê²Œ ë‚˜ì˜¤ëŠ” ê²ƒì¸ê°€ìš”?
    - At lower packet rates, the number of packets processed during each interrupt is smaller, leading to higher CPU usage per packet [CPU Usage](https://www.notion.so/CPU-Usage-7e0945e184c94f5ea3e3fb3e3746224b?pvs=21)