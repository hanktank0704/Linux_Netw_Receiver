발표자 - 김명수

논문 - TCCL: Discovering Better Communication Paths for PCIe GPU Clusters

---

GPU끼리 communication Path를 설정해서 잘 통신할 수 있게 해줌

요즘은 멀티 GPU를 사용하고 있음.

parallel하게 연결하여 속도 향상을 꾀함.

따라서 GPU 통신 속도도 중요해지게 됨.

collective communication libraries를 각 제조사마다 만들어서 잘 사용 중임.

High bandwidth systems는 GPU끼리 NVLink로 연결되게 됨.

PCIe-dependent systems는 CPU를 거쳐서 가야함.(안거쳐도 되긴함.)

PCIe Switch로 여러대의 GPU를 물리고 있었음

대부분은 High bandwidth 만 연구되어 있고, PCIe-dependent는 많이 사용되고 있지만 연구가 잘 되어있지 않았음. 따라서 사용율에 비해 연구가 덜 되어있다고 분석하였음.

기대값보다 성능이 잘 나오지 않았음.

누마노드간이 통신 때문에 성능하락 / CPU 메모리를 거쳐서 GPU간에 데이터 이동이 있을 때 적은 바운스 버퍼가 성능하락을 일으킴. ⇒ 분석해서 나온 결론.

collective communication → 병렬 컴퓨팅에서 프로세스간에 데이터를 교환할 때 쓰이는 개념. 상당히 범용적이다.

Ring vased ReduceScatter : 각각의 작업을 각각의 GPU에 나누어서 병렬처리를 하고 이를 Reduce하게 됨.

처음 상태가 아니라 중간상태로 이동을 최대한 줄이면서 결과값을 얻을 수 있게 해주는 느낌이였음.

AMD EPYC cpu는 다중 SoC dies임. 따라서 CPU는 하나지만 이를 가지고 4개의 NUMA 노드가 있다고 볼 수 있음.

프로파일러를 통해 최적의 가장 좋은 Path를 찾아서 해당 Path로 communication을 수행하게 됨.

작업이 처리되기 전에 오버헤드가 일부 존재하고 이후에는 없으므로 작업을 하는데 있어서 직접적인 오버헤드는 존재하지 않는다.

커뮤니케이션 알고리즘의 토폴로지를 바꾸게 됨.

### 프로파일러

---

기존의 벤치마크 툴들은 각각의 패스에 대하여 동시에 여러개의 프로그램을 실행하였음.

이는 부정확한 결과값을 초래함.

Initial phase 이후에 서로 Sync를 하고, 이를 끄지 않고 이를 pool로 만들어서 다시 Init하는 오버헤드를 줄이게 됨.

---

성능저하의 원인이 정형적이지 않다고 판단되었음. 그래서 다익스트라 알고리즘을 사용하게 되었다고 추정

congestion이 internode에서만 발생하고, intranode에서는 별로 발생하지 않았다고 보았음.