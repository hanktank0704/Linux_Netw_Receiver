
### 지난 스터디 메모
---
그날의 발표자를 정할 것.
발표자 - 황재훈
netif -> network interface, 어떻게 발음할 것인가?
그래프 뷰를 좀 더 잘 사용할 수 있는 방법을 찾아볼 것.
softnet_data에 대하여 좀 더 조사할 것. napi 관련하여 stat을 저장하는 구조체
softnet_data 등장 시점을 확인해 볼 것. 파악을 확실하게 전부 조사해볼 것.
구조체가 어떻게 선언되어 있는지.

eBPF에서 쓰이게 되는데, trace_~ () 등의 후킹 포인트가 된다.

네트워크 관리하는 큐의 갯수는 CPU 코어의 갯수와 같다.
rps 코드 분석하기.

같은 flow에서 

RPS 로드 밸런싱을 할 수 있는 방법을 구체적으로 찾아볼 것.
문제는 없는지 고민해 볼것.

struct softnet_data `*mysd` = this_cpu_ptr(&softnet_data) -> 어떻게 가져오는지 확인할 것.

비트가 켜져있을 때 net_rx_action이 어느 코어에서 실행되는지 정하는 로직
poll_list, input_packet_queue
### 주요 토픽 
#### 1. RPS
---
요약 : 

[[get_rps_cpu() 함수에 대한 로직 분석과 고찰 및 의의]]

[[RPS-IPI 관계도]]

#### 2.napi_schedule_rps
---
napi_schedule_rps(sd) =>다른 코어의 IRQ를 보내지 않는다. 단순히 현재 내 CPU에 softnet_data를 불러와서 ipi_list에 새로 깨워야 할 CPU를 추가할 뿐이다. 이 로직에 쓰이는 member variable은 크게 두 개이다.
`rps_ipi_next`와 `rps_ipi_list`가 그러한데, 둘 다 softnet_data를 가르키는 포인터 이다. 우선 `rps_ipi_list`는 해당 cpu에서 rps를 처리하면서, 패킷을 넘겨주어 깨워야할 cpu들의 linked list를 가르키는 포인터이다. 그 다음부터는 `rps_ipi_next`를 통해서 계속 softnet_data 타입의 구조체가 연결되게 되고, 이 목록들은 처음 softnet_data가 만들어낸 새로 깨워야 할 cpu 목록인 셈이다.

rps_eable()
-> smp ~ function ~ 가 다 해줌.

net_rx_action()
net_rps_action_and_irq_enable()
net_rps_send_ipi()
net/core/dev.c에 있음


#### 3. IPI sequence
---
**smp vs up (symmetric multiprocessing vs uni - processing)**

###### route of IPI process
/kernel/smp.c

smp_call_function_single_async()

generic_exec_single()

`__smp_call_single_queue()`

send_call_function_single_ipi()

arch_send_call_function_single_ipi()

/arch/x86/include/asm/smp.h

smp_ops.send_call_func_single_ipi()

/arch/x86/kernel/smp.c

.send_call_func_single_ipi = native_send_call_func_single_ipi

/arch/x86/kernel/apic/ipi.c

native_send_call_func_single_ipi()

/arch/x86/include/asm/apic.h

`__apic_send_IPI()`

apic -> send_IPI()

/arch/x86/kernel/apic/probe_32.c

apic_default.send_IPI = default_send_IPI_single

/arch/x86/kernel/apic/ipi.c

default_send_IPI_single()

/arch/x86/include/asm/apic.h

`__apic_send_IPI_mask()`

/include/linux/static_call_types.h
`static_call_mod() == __raw_static_call() == &STATIC_CALL_TRAMP(name)`

&STATIC_CALL_TRAMP(name)  == ` __PASTE(STATIC_CALL_TRAMP_PREFIX, name)`

`__PASTE(a,b) = a##b`  << 얘는 컴파일러한테 두 토큰을 합치라고 알려줌. 즉 return은  ab가 됨.

STATIC_CALL_PREFIX는 `__SCT__`이므로, 결론적으로 `__SCT__name`을 반환하게 됨.

apic_call_send_IPI_mask() == apic->send_IPI_mask()

/arch/parisc/kernel/smp.c

send_IPI_mask()

ipi_send()

/arch/parisc/include/asm/io.h

gsc_writel()
-> 어셈블리어로 직접 작성하고 있었음.
```c
static inline void gsc_writel(unsigned int val, unsigned long addr)
{
	__asm__ __volatile__(
	" stwas %0,0(%1)\n"
	: : "r" (val), "r" (addr) );
}
```



#### 4.softnet_data
---

[[softnet_data]]
softnet_data는 `/include/linux/netdivice.h`에 있음

![[Pasted image 20240816140918.png]]

`/net/core/dev.c` 에서 보면 `net_dev_init()`함수 속에 csd를 초기화하는 `INIT_CSD`매크로 함수가 있다. 2번째 parameter를 `func`으로, 3번째 parameter를 `info`로 하여 `__call_single_data`타입으로 캐스팅하여 이를 주어진 csd 포인터에 매핑한다.

```c
/* Called from hardirq (IPI) context */
static void rps_trigger_softirq(void *data)
{
	struct softnet_data *sd = data;
	  
	____napi_schedule(sd, &sd->backlog);
	sd->received_rps++;
}
```
보시다시피 해당 `sd`와 그 `sd`가 저장하고 있는 `backlog` napi struct를 가지고 `____napi_schedule()`을 호출하게 된다. 이는 ipi를 통해 이루어진 hardirq에서 실행되게 된다.

처음 NIC의 하드웨어 인터럽트에 의해 실행된 `net_rx_action`의 경우에는 rps를 통해 각각의 CPU로 steering을 하게 된다. 만약 그 CPU가 오프라인이거나 할당된 cpu가 없을 경우 해당 인터럽트를 받은 cpu가 그대로 프로토콜 처리까지 맡게 되는 것이다. 그게 아니라면 softnet_data의 패킷 큐에 해당 패킷을 추가하게 되고, 이는 
#### 5. etc
---

enqueue_to_backlog()를 호출하는 함수들.
netif_rx_internal()
netif_receive_skb_internal()
netif_receive_skb_list_internal()

- net_rx_action 추가 설명 서술 [[Encyclopedia of NetworkSystem/Function/net-core/net_rx_action()|net_rx_action()]]
- 전체 함수 콜스택 도식화
- `ice_32byte_rx_desc`, `ice_32b_rx_flex_desc` 도식화 [[주요 개념 구조도]]
- `list_head`관련 유의사항 - `list_head` 타입의 멤버변수는 리스트의 헤드가 될 수도 있지만, entry로도 사용되기 때문에 코드 맥락을 살펴보며 정확한 usage를 파악해야 한다.
- `__netif_receive_skb_list()` 추가 설명 서술
- [[`__netif_receive_skb_core() 함수에 대한 로직 분석과 고찰 및 의의]]
- 
### 참고자료
---
https://wariua.github.io/facility/long-list-of-linked-lists.html
리눅스에서 구현한 여러가지 linked list에 대한 분석

https://people.redhat.com/pladd/MHVLUG_2017-04_Network_Receive_Stack.pdf
현재 관심사와 비슷한 내용의 ppt

