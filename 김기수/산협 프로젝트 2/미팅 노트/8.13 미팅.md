#### 지난 스터디 메모
---
황재훈 - flush 변수만 확인, same flow만 확인하게 됨. 따라서 바꾸는데는 상당한 소요가 필요함. 찍어보는게 확인이 필요함.
ice_build_skb vs ice_construct_skb -> xdp->data 부분을 활용하냐 안하냐
pfmemalloc - slab, slub, slob alloc 등이 있음. 자세한 확인이 필요해 보임. 공부해보면 좋을 듯.
headlen - data에 있는 내용 길이, datalen - frags에 있는 내용 길이, len - 전체 프레임 길이

박찬서 - L3와 L4를 바꾸었을 때 성능차이를 중점적으로 확인 함.
좀더 정보를 가져오자.
Flamegragh를 그려볼 것. 각각의 함수가 어느정도의 potion을 가지고 있는지 확인해 볼 것.

rps 가기.

#### 주요 토픽
----
- xdp를 어차피 쓸거 xdp에서 userdata만 올려버리고 헤더를 네트워크 스택을 타게 한다. -> dma를 userspace로 다이렉트로 때려버리고, 이를 접근하기 위한 소켓 함수를 AF_INET()함수가 AF_XDP() 함수를 감싸게 한다.

- 링 디스크립터가 dma 주소를 세팅 받는지 여부를 확인하였다. `ice_alloc_rx_bufs`에서 보면 `rx_desc->read.pkt_addr`를 해당 `ice_rx_buf`의 `dma + page_offset`값으로 세팅하는 부분이 있다. 따라서 device가 각각의 페이지를 접근할 수 있게 되는 것이다. 이는 또한 `skb`를 만들 때, `xdp`가 설정될 때의 `hard_start` 주소가 될 것이다.
  [[주요 개념 구조도]]
  
-  3계층 4계층 5계층 함수 호출관련 참고자료 [링크](https://os.korea.ac.kr/wp-content/uploads/2020/12/10_IP-Implementations.pdf)

- ![[Pasted image 20240812173405.png]]
- perf를 찍어봤는데, iperf3를 기준으로 둘의 오버헤드는 큰 차이가 없었다.
  여기서 고려 되어야 할 점은 iperf3가 돌아가면서 찍히는 심볼 들만 보인다는 점이다. 즉, 다양한 flow가 아닌 iperf3만의 flow가 들어왔을 것이므로, `gro_list`의 길이는 항상 1으로 고정되고, 들어오는 패킷들은 순차적으로 gro에 의해 병합될 것이다. 타켓 트래픽 패턴을 특정하고, 이와 부합하는 데이터를 따로 모아야 할 것으로 보인다.

- 또한 저 perf 결과에 대한 해석으로, 해당 함수가 콜스택의 최상단에 있을 때를 기준으로 계산한다고 보았다. FlameGraph를 그리려고 했는데, 옵션이 잘못 들어가서 인지 그려지지 않아 `perf report`를 통해 위와 같은 결과를 얻을 수 있었다.


- RPS는 dev_gro_receive()에서보면, `netif_receive_skb_list_internal()`함수에서 `ifdef CONFIG_RPS` 매크로를 통해 실행되게 된다. 이때 RPS란 다음 링크에서 자세하게 확인할 수 있다. 
	[[RSS - receive side scaling]]
[[Encyclopedia of NetworkSystem/Function/net-core/dev_gro_receive()|dev_gro_receive()]]
[[The SLAB allocator]]

netif_receive_skb_list_internal() 이후에 RPS가 설정되어 있다면 꺼지는 부분이 있는지 확인할 것.

#### 스터디 메모
---
그날의 발표자를 정할 것.
발표자 - 황재훈
netif -> network interface, 어떻게 발음할 것인가?
그래프 뷰를 좀 더 잘 사용할 수 있는 방법을 찾아볼 것.
softnet_data에 대하여 좀 더 조사할 것. napi 관련하여 stat을 저장하는 구조체
softnet_data 등장 시점을 확인해 볼 것. 파악을 확실하게 전부 조사해볼 것.
구조체가 어떻게 선언되어 있는지.

eBPF에서 쓰이게 되는데, trace_~ () 등의 후킹 포인트가 된다.

네트워크 관리하는 큐의 갯수는 CPU 코어의 갯수와 같다.
rps 코드 분석하기.

같은 flow에서 

RPS 로드 밸런싱을 할 수 있는 방법을 구체적으로 찾아볼 것.
문제는 없는지 고민해 볼것.

struct softnet_data `*mysd` = this_cpu_ptr(&softnet_data) -> 어떻게 가져오는지 확인할 것.

비트가 켜져있을 때 net_rx_action이 어느 코어에서 실행되는지 정하는 로직
poll_list, input_packet_queue

---

napi_schedule_rps(sd) =>다른 코어의 IRQ를 보내지 않는다.

rps_eable()
-> smp ~ function ~ 가 다 해줌.


net_rx_action()
net_rps_action_and_irq_enable()
net_rps_send_ipi()
smp_call_function_single_async()
softnet_data -> csd ->func을 실행하게 됨. 특정 CPU 에서.
