#### 지난 스터디 메모
---
- 황재훈 - dev_gro_receive() 조사 관련 발표.

초록색으로 색칠되어있는부분을 비교하여 같은 flow인지 확인하게 됨.

xdp에서 pass가 되는 경우를 자세하게 살펴볼 것.

저 포인터가 가르키고 있는 공간에는 어떤 데이터를 가지고 있는가?

아마 rx_buf를 포인팅 하고 있을텐데 이걸 정확하게 할 것.

raw data를 직접 DMA를 통해 메인 메모리에 저장함.

skb를 비교할 때 메타데이터를 비교할려면 미리 셋팅이 되어있어야 함. 이거는 혹시 오버헤드가 있는지 확인해볼 것.

offloading → stand-alone한 작업 위주로. • Page pool infrastructure as the base layer

ack nr이 같은지 확인한다. → 만약 client가 host에게 무언가를 보냈을 경우 state의 변화가 있을 수 있으므로 합치지 않게 됨.

tcp에서인지 ip에서인지 문제를 확인해 보자 더 많은 것을 앞으로 해서 오버헤드를 조금이라도 줄일 수 있을 것이다.

커널 로그 찍는 법 B2, A2 서버를 확인해 보면 좋을 듯.

글로써 정리하는 것을 연습할 것.

gro_normal의 의미를 확인할 것.

함수들 간의 기능 비교 해볼 것.

Rps, rfs dev까지 왔을 떄 어디로 넣어주는가를 확인할 것.


#### 주요 토픽
----
 1. 의문 -> 만약 napi polling 속도보다 ring buffer에 쌓이는 속도가 더 빠르면 이것 또한 bottleneck이 될 수 있지 않은가? 이것을 속도를 직접 측정해 볼 수 있을까? 아니라면 CPU IPC 성능과 한번에 폴링되는 갯수를 가지고 이론적으로 계산해볼 수 있을까? 만약 그렇다면 CPU 성능과 선형적인 연관관계를 가진다면 오늘날의 PC 컴퓨팅 성능이 이를 커버할 수 있을 정도로 적절한가?

2.  XDP와 관련하여.
	`xdp_buff` 타입의 `xdp`는 xdp program의 input으로서 사용하게 됨. 이러한 xdp program의 ret값으로 `ice_rx_buf->act` 멤버에다가 해당 패킷의 행동을 결정짓는 enum을 넣게 됨. 이것을 셋팅하는 것이 `ice_set_rx_bufs_act()`이다.
	 만약`xdp->data`가 `null` 값이라면 `rx_buf`의 page를 직접 data 영역으로 설정하게 된다. 따라서 copy가 일어나지 않는다.
	 
		![[Pasted image 20240801110703.png]]
		`xdp_buff`의 구조 개요도
		출처 - https://people.redhat.com/lbiancon/conference/NetDevConf2020-0x14/add-xdp-on-driver.html
	
	 `sk_buff`에서 새로운 data 영역을 선언한 부분은 `page_frag_alloc()`함수의 `return` 값인 `address`로, 이는 저번 미팅 때 새로운 메모리를 할당 받은 것으로 착각하였으나, `page_frag_cache`라는 구조체 안에서 가져오게 되는 page 주소이다. 이는`napi_alloc_cache`의 멤버 변수로 선언되어 있다.
	 ice_txrx.c의 1067번째 줄을 보면, `ice_construct_skb()` 함수에서 `memcpy` 함수가 들어있는 것을 볼 수 있다.
	 또한 추가적으로 ice_xsk.c의 919번째 줄을 보면, `ice_construct_skb_zc()` 함수에도 `memcpy` 함수를 확인할 수 있었다. 다만 여기서는 metadata만 `memcpy` 하는 것으로 보였다.
	 
	 여기서 정확하게 `ice_build_skb`와 `ice_construct_skb의` 차이점을 확인 할 수 있었는데, 전자의 경우 xdp가 가르키고 있는 data 영역을 다시 포인팅하여 skb를 만드는 것이고, 이는 그 `ice_clean_rx_irq` 에서 볼 수 있듯이 `likely`로 if문 안에서 나타나고 있어 주로 `ice_build_skb`로 이루어짐을 알 수 있다. `ice_construct_skb`도 코드는 있으나, 주요하지 않은 경로임을 확인할 수 있었다.
	 
	 *부가정보*
	 `page_frag_cache`는 해당 CPU에 대한 `netdev_alloc_cache`를 가르키는 포인터를 가져오게 되고, `napi_alloc_cache`는 해당 CPU에 대한 `napi_alloc_cache`를 가르키는 포인터를 가져오게 된다. 이 때 `napi_alloc_cache`라는 구조체를 잘 살펴보게 되면, `page_frag_cache` 구조체와, `skb_cache`를 가르키는 `void *` 배열을 가지고 있음을 알 수 있다. 이 `void *`는 `skb`를 저장할 수 있는 포인터이다.

3. 메모리 할당의 분류
   1) 링은 `kzalloc`으로 설정 됨. - `ice_probe()` 콜 스택 중 `ice_vsi_alloc_rings()` 중에 설정
   2) 링 버퍼들은 `kcalloc`으로 설정 됨. - `ice_open()` 콜 스택 중 `ice_vsi_setup_rx_rings()` 중에 설정
   3) 링 디스크립터들은 `dmam_alloc_coherent()`로 설정 됨. - `ice_open()` 콜 스택 중 `ice_vsi_setup_rx_rings()` 중에 설정. desc는 `void *`타입이며, `/*Descriptor ring memory*/`라고 설명이 쓰여져 있음. 이 주소는 dma 주소를 가지게 됨. 이 dma 주소는 ring 구조체의 field에 존재하게 됨.
   4) 링 버퍼의 page들은 `dev_alloc_pages()`와`dma_map_page_attrs()` 로 설정 됨. -`ice_open()` 콜 스택에서 `ice_vsi_setup_rx_rings()` 다음으로 나오는 `ice_vsi_cfg_lan()` 함수 내부에 `ice_vsi_cfg_rxqs()` 내부에 `ice_alloc_rx_bufs()` 함수로 실행되게 됨. 이 함수는 ice_txrx.c 내부에 있으며, `ice_alloc_mapped_page`를 통해 기존의 페이지가 있다면 계속 사용하고 아니면 `dev_alloc_pages`를 통해 새로운 페이지를 할당 받아 이를 매핑하고, `dma_map_page_attrs()` 함수를 통해 dma 매핑을 하게 된다. 이 dma 주소는 해당 buf의 dma field에 저장되게 된다.

4. `INDIRECT_CALL_` 매크로 사용 규칙
   1) `INDIRECT_CALL_INET`은 IPv4, IPv6를 구분하여 서로 다른 함수를 호출하고 싶을 때 사용하게 된다. 이는 `INDIRECT_CALL_2`라는 매크로의 이명이며, 이 또한 `include/linux/indirect_call_wrapper.h`에 모두 정의되어 있다.
   2) `INDIRECT_CALL_$NR` 매크로의 경우 `$NR`개의 함수들이 정의되어 있을 때,  첫 번째 함수에 해당하는 함수가 실행 될 수 있도록 indirect call을 통해 실행해준다.
   3) 즉, `INDIRECT_CALL_2`는 `INDIRECT_CALL_INET`과 같은 함수이다.
   4) 만약 뒤의 함수 포인터 리스트와 아무것도 맞지 않다면 맨 앞에 함수를 실행하게 된다.

5. `Control Block`에서의 `flush`의 의미
   1)`dev_gro_receive`에서 `flush`멤버의 경우 `skb`가 `frag_list`를 가지는지 여부로 비트를 설정했다.
   2) `inet_gro_receive`에서 각각의 `p` 와  `skb` 간에 `ttl`, `tos`, 그리고 `fragment`되었는지의 일치 여부를 `flush`에다가 저장한다.
   3) 

6. `gro_normal_one`의 의미
  1) `gro_normal_one`이 실행되는 곳 -`napi_gro_complete()`중 해당 skb->count == 1인 경우, `napi_gro_receive()`에서 `napi_skb_finish()`중 return 값이 GRO_NORMAL인 경우, 마지막으로 `napi_frags_finish()`에서 이다. 이때, 마지막 `napi_frags_finish()`는 `napi_gro_frags()`에서 호출되게 되는데, 이와 `napi_gro_receive()`호출은 선택적이고, ice에서는 `napi_gro_frags()`는 호출되지 않았으므로 여기서는 고려대상이 아니다. 두 함수가 있는 원인은 추후 조사가 더 필요로 하다.
     
  2) `dev_gro_receive`에서 보면 `normal`라벨로 가는 부분이 있다. 이것이 `netif_elide_gro(skb->dev)`가 참이거나, 혹은 해당 패킷에 해당하는 `gro_receive`콜백 함수가 없을 경우에 가며, 이후 다시 `ok` 라벨로 거꾸로 올라가는 것을 보아 이는 gro를 할 수 없는 패킷들에 대하여 return을 `GRO_NORMAL`로 설정해주고, 혹시 바로 볼 수 있는 `frag`가 없다면 전체 `frag`들을 한칸씩 앞으로 땡기는 `memmove`함수를 호출하는 `gro_try_pull_from_frag0()`함수를 실행하게 된다.
     
  3) `ok`라벨에서는 활성화 혹은 비활성화 된 `gro_list`들에 대하여 napi의 `gro_bitmask`가 이를 반영하도록 비트를 셋팅해주고, 결과 값을 리턴하게 된다. 이 함수는 기본 경로에서도 똑같이 실행된다.
     
  4) `~_gro_receive` 함수들에서 `NAPI_GRO_CB(p)->same_flow`와 `NAPI_GRO_CB(skb)->same_flow`가 차이가 있다. `p`는 gro_list에 들어있는 skb들을 순서대로 꺼내서 비교할 때 쓰는 변수로, 여기서 사용되는 의미는 지금 비교하고 있는 새로운 skb와는 다른 flow임을 0과 1을 통해 표현한다. 다음으로 `skb`는 새로 만들어진 skb로, 여기서의 `same_flow`는 내가 지금 gro_list 중에 합칠 게 있었고, 합쳤어. 를 나타내주고 있다. 따라서 콜 스택 최층부인 `skb_gro_receive`가 끝나더라도 이러한 결과는 `same_flow`에 저장되어 있는 것이다. 이를 바탕으로 작업이 계속 이루어지게 되는 것이다.

```c title=gro_result
enum gro_result {
	GRO_MERGED,
	GRO_MERGED_FREE,
	GRO_HELD,
	GRO_NORMAL,
	GRO_CONSUMED,
};
typedef enum gro_result gro_result_t;
```

7. 기타
  1) `napi` 통째로 상위 스택으로 넘어가게 되는데, 이때 `napi->rx_list` 리스트가 따로 뽑혀서 넘어간다. 이는 특정 길이 이상 `gro_list`가 채워졌을 경우에 실행된다.
     
  2) `inet_gro_receive`에서 `gro_list`를 돌면서 각각의 `skb`에 대한 `NAPI_GRO_CB(p)->same_flow`를 설정하게 된다. 여기서 같으면 1, 다르면 0이 된다. 따라서 해당 `skb`에 대한 `same_flow`여부는 `p`를 돌면서 `same_flow`인지만 보면 된다.
     
  3) `skb_gro_receive`에서 `merge`로 빠지는 경우는 `frag` 갯수가 부족한 경우이다. 이 때는 직접 합치는게 아니라 `skb_frag_off_add`, `skb_frag_size_sub`등의 함수를 사용하여 크기를 조절하고, 해당 패킷의 마지막이 본인이라면, `frag_list`에다가 합치려는 `skb` 자체를 넣으면 되고, 그게 아니라면 `napi_gro_cb->last->next`에 합치려는 `skb`를 넣으면 된다. `frag_list`와 `frags`는 많이 다르다. `frag_list`는 `sk_buff *` 타입으로, 리스트처럼 쓸 수 있다. 그러나 `frags`같은 경우에는 `skb_frag_t` 타입의 정적인 array로, `MAX_SKB_FRAGS`갯수 만큼의 array로 선언되어 있다. 여기서 `MAX_SKB_FRAGS`는 ice의 경우 `17`로 정의되어 있다. *(skbuff.h 350번째 줄)* `skb_frag_t`의 경우 핵심은 `netmem_ref`인데, 이는 `unsigned long __bitwise`로 정의되어 있다. 또한, 여기에는 `page`를 항상 참조하도록 되어 있어 사실상 페이지 조각을 가르키고 있다고 보면 된다.
     
  4) `NAPI_GRO_CB(skb)->free`는 멤버 변수로 `NAPI_GRO_FREE` 혹은 `NAPI_GRO_FREE_STOLEN_HEAD`를 가질 수 있다. `skb_gro_receive`를 마저 보면, 만약 `headlen <= offset`이라면 `frag` 를 직접적으로 주소를 옮겨줌으로써 패킷을 합치고 `free`값이 `NAPI_GRO_FREE`가 된다. 아니라면 만약 `skb->head_frag`가 설정되어있는 경우 `free`값은 `NAPI_GRO_FREE_STOLEN_HEAD`로 설정된다. 그러나 `dev_gro_receive`함수에서 `ret`값을 이 `free`변수의 유무로 판단하여 `GRO_MERGED_FREE` 아니면 `GRO_MERGED`로 나뉘게 된다.

8. TCP / IP 오버헤드 성능 측정과 관련 된 고찰
  1) gro 부분에서 둘의 순서를 바꾸었을 때, 오버헤드가 얼마나 줄어드나? -> 매번 Layer 3 length를 측정하고 Layer 4를 접근한다고 했을 때 이것이 더 좋은 성능을 보여줄 수 있는 것인가?

#### 정리한 페이지 노트들
---
[[Iperf3 - Can the Linux networking stack be used with very high speed applications]]

[[Page fragments]]

#### 스터디 메모
---
황재훈 - flush 변수만 확인, same flow만 확인하게 됨. 따라서 바꾸는데는 상당한 소요가 필요함. 찍어보는게 확인이 필요함.
ice_build_skb vs ice_construct_skb -> xdp->data 부분을 활용하냐 안하냐
pfmemalloc - slab, slub, slob alloc 등이 있음. 자세한 확인이 필요해 보임. 공부해보면 좋을 듯.
headlen - data에 있는 내용 길이, datalen - frags에 있는 내용 길이, len - 전체 프레임 길이

박찬서 - L3와 L4를 바꾸었을 때 성능차이를 중점적으로 확인 함.
좀더 정보를 가져오자.
Flamegragh를 그려볼 것. 각각의 함수가 어느정도의 potion을 가지고 있는지 확인해 볼 것.

rps 가기.
